{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097e4c9e",
   "metadata": {},
   "source": [
    "NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213c8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will discuss a higher level overview of the basics of Natural Language Processing, which basically consists of combining \n",
    "# machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning \n",
    "# algorithms can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b697170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    # This is done after we import the nltk library in the command prompt using pip install nltk or in anaconda by \n",
    "               # using conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3a5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "Hit Enter to continue: \n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "Hit Enter to continue: \n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "Hit Enter to continue: \n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [*] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "Hit Enter to continue: \n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet2022......... Open English Wordnet 2022\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "Hit Enter to continue: \n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> q\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()  # nltk comes with a bunch of datasets which are necessary in order for it to operate in certain ways.\n",
    "                       # when we call this we can see that there is a little interactive shell that we can use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743df2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, if we enter l and press enter in the initial box we get a list of packages , and we can keep pressing enter to continue \n",
    "# through the list. We are just going to download one of these packages , but we could just use .download to download all of \n",
    "# them.\n",
    "# We are actually looking for the words package which is the stopwords or Stopwords Corpus package. We continue to hit enter\n",
    "# unitl we reach the end of all the packages and we get the initial interatctive Downloader box back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55477341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Downloader box we write d for download - press enter and then tell it which package we want to download, in the \n",
    "# identifier box, which is the box we get after we enter d.\n",
    "# Note: Initially we got options like Download, List, Config,Help Quit and each letter does exactly that like d is for download\n",
    "# and l is for list and so on....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129f7a2",
   "metadata": {},
   "source": [
    "Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17308eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using a dataset from the UCI datasets! \n",
    "# The file we are using contains a collection of more than 5 thousand SMS phone messages.SO we want to filter out if its spam \n",
    "# or actually important messages.Using this data we are going to build a spam detection filter using python.\n",
    "# Let's go ahead and use rstrip() plus a list comprehension to get a list of all the lines of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c88438",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]  # Here we say messages(variable) = and we \n",
    "                                                                                    # use some list comprehension i.e \n",
    "                                                                                    # [line.rstrip()...open('')] and within\n",
    "                                                                                    # open and we pass in the actual file path\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044e8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(messages))   # Now if we check out the length of the variable messages we get 5574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9b9f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]  # And if we check out one of the items in this 'messages' its the actual string of the first message.\n",
    "             # We can put in any number to get the relevant message for that number like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47357e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tWhat you thinked about me. First time you saw me in class.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23428dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A collection of text is sometimes called a corpus.Let's print out the first 10 messages and number them using enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76321eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mess_no,message in enumerate(messages[:10]):    # Here we created a for loop that contains the message number and the \n",
    "    print(mess_no,message)                          # message and we use the enumerate function to obtain the messages from \n",
    "    print('\\n')                                     # 0 to 9 which is the first 10 messages and then we go ahead and print\n",
    "                                                    # both the message number and the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da585189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at these text messages and their style we can clearly see that some of them are spam. There are some which are \n",
    "# labelled spam while the others are labelled ham. The one's labelled spam if we read them we can see that they are mostly ads\n",
    "# What we want to do is figure out which messages are spam v/s the one's that are real in the actual text messages that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8744d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the spacing we can actually tell if this is a tab seperated values file or tsv , where the first column is a label where\n",
    "# we have a given message probably known as ham or spam and the second column is the message itself. What we mean by that is \n",
    "# if we go ahead and read just one of these messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d480d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d72ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have ham\\t and then the actual message. The \\t indicates that it is a tab seperation. And instead of just \n",
    "# parsing the tsv or tab seperated file using python we use our pandas knowledge to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d66eebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "481c3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the read_csv and make note of the sep argument and we can also make of the column names by passing in a list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "045875dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names = ['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "246a3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, here we say messages = pd.read_csv('') and we pass in the actual file location and here we are going to mention the \n",
    "# seperator is a tab by saying '\\t and the names for these columns is the label and then the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a6feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ed181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see that we have a nice dataframe with the label and the message string seperated for us to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bac6f",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dd8f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9fd7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see that we have 2 unique labels which makes sense ('ham' and 'spam'), but what is interesting is that the count\n",
    "# of the messages are more(5572) as compared to the unique messages (5169) so that means there are some repeated messages which\n",
    "# also makes sense as there might be messages like 'Sorry,I'll call later', 'hey','yes','no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "924d343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now what we can do is use groupby to decribe by label,this way we can start thinking about the features that seperate ham and\n",
    "# spam. We are going to get a higher level view of the data just to get an idea of what seperates a ham message from a spam\n",
    "# message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e70bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()   # So, we are grouping by the label column which is 'ham' or 'spam' and then applying\n",
    "                                       # describe to that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6550d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get an idea of how many ham messages we have and how many spam messages. The most popular ham message is 'Sorry,I'll\n",
    "# call later', the most popular spam message is \"Please call our customer service....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b76305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the \n",
    "# general idea of feature engineering. The better your domain knowledge on the data, the better your ability to engineer more\n",
    "# features from it. Feature engineering is a very large part of spam detection in general. I encourage you to read up on the topic!\n",
    "\n",
    "# Let's make a new column to detect how long the text messages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ad91ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)   # Here we select the 'messages' column from messages and we apply the \n",
    "                                                      # built in length function i.e apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0e779db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()  # Now we have the label, the message and the actual length of the string message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c0142",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81079752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215d4bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages['length'].plot.hist(bins=150)  # Here we are plotting the length of the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe19dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see a bimodal behaviour there's a spike initially,at the lower end  followed by another spike after sometime, \n",
    "# agian at the higher end but it looks like text length maybe a feature to think about. If we see that the x axis extends to \n",
    "# about 1000 which means that there are some really long text messages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26ba3560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'].describe()  # We can see here the max is 910 which is pretty close to 1000 and is quite a large message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6a8227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's go ahead and find this message using pandas masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9e469b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>ham</td>\n",
       "      <td>For me the love should start with attraction.i...</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length\n",
       "1085   ham  For me the love should start with attraction.i...     910"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length']==910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f2b6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gives us this row with this message and the ... indicates there's a much longer message than what we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23bd5a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length']==910]['message'].iloc[0] # We do this so we can read the entire message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79e0cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will go ahead and see if length is a distinguishing feature between ham and spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "229de702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:title={'center':'ham'}>,\n",
       "       <AxesSubplot:title={'center':'spam'}>], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length',by='label',bins =60,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0572bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have made a histogram of messages so we do messages.hist(), and we do something similar to what we have done in seaborn to\n",
    "# specify the columns, meaning I can add subplots based off the column so we add column='length'.\n",
    "# So,now we can see ham vs spam in 2 seperate columns the length is shown on the x-axis and is seperated by the label column.\n",
    "# It's pretty interesting to see that just through EDA we have uncovered the fact that spam messages tend to have higher \n",
    "# charecters \n",
    "# If we look at the ham text messages the range is between 0 to around 180 or 200 with an average at around 50\n",
    "# while, the spam text messages have an average at around 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19825320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at these graphs it looks like length is an important feature to help distinguish between ham and spam messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f078c",
   "metadata": {},
   "source": [
    "Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6a52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main issue with our data is that it is all in text format (strings). The classification algorithms that we've learned \n",
    "# about so far will need some sort of numerical feature vector in order to perform the classification task. There are actually \n",
    "# many methods to convert a corpus or strings to a vector format. The simplest is the the bag-of-words approach, where each \n",
    "# unique word in a text will be represented by one number.\n",
    "\n",
    "# In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9702a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step we will write a function that will split a message into its individual words and return a list. We are also\n",
    "# going to remove the common words like 'the','and','if' etc...these are known as stop words and that's where we will be using \n",
    "# the nltk library, that we saw earlier.\n",
    "# We are also going to take advantage of python's built in string library so we do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5380e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d195788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the first thing that we want to do is remove punctuation. We are going to create a sample message called mess to show how\n",
    "# this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e207c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = 'Sample Message! Notice: It has punctuation.' # So, this is the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dd7b8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation   # Notice when we do this we have a string of various punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e0c1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now what we can do is use list comprehension in order to pass in to every charecter and check if its not in the string\n",
    "# punctuation like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dc7403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = [c for c in mess if c not in string.punctuation] # nonpunc contains charecters that are not in string.punc and in mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa7067b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'M',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'I',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc  # This basically contains just the individual charecters seperated without the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6e603704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see how we can remove stop words like 'if','and', 'the'... We can import a bunch of stop words from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63f3aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50160941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')  # If we run this this will give us all the stop words in english. These are just very common words\n",
    "                            # that don't really tell us any distinngusihing features. These are such common words that we are \n",
    "                            # trying to do a spam or ham detection or any type of classification on text data , these words \n",
    "                            # are so common that they are probabaly not going to be very helpful as far as distinguishing one \n",
    "                            # source of text from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fb8232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's go ahead and grab nopunc again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8a9f7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'M',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'I',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc  # And since this is just a list of letters we do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd845ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = ''.join(nopunc)  # And this is essentially just joining the elements of a list together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26f9673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample Message Notice It has punctuation'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc   # Now if we do nopunc we notice it is back together but its's still has no punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6599bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's quicky go over what is going on with .join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e67ae227",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a','b','c','d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad47b19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x   # So we have x which is a list of a,b,c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ea5408f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we do .join and pass in a list of elements it joins them and whatever we want in the '' before joining them it will use \n",
    "# it as a concatination device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34a2813d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a++++b++++c++++d'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'++++'.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e834212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47053797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a____b____c____d'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'____'.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07c0bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is essentially what happened here as well when we did nonpunc = ''.join(nopunc) it basically just joined the indiviual \n",
    "# elements of nopunc with just the spaces as we did't enter anything within those quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0069d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's go ahead and grab nopunc again and we split that gain so we have a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c1f1a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'Message', 'Notice', 'It', 'has', 'punctuation']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cb0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a list of all the words and we can use list comprehension again to remove any stop words.\n",
    "# So, we say clean_mess and using list comprehension we then lower all the words in nopunc.split so we don't have to deal with\n",
    "# the case and we put the condition that only the words that are not in stop.words('english') but are there in nopunc.split\n",
    "# will be a part of clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88f6c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7af9451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'Message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess   # Now we can see that all the stop words in nopunc.split have been removed from clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "449867a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put both of these together in a function to apply it to our DataFrame later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4008712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now going to remove the stop words in the message column by creating a function that will help us do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdb3fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation.i.e only if the charecter is not a punctuation it's considered\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4abba793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have the function which is basically the three steps that we did earlier. \n",
    "# Now we are going to check out messages just the head of that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "089b0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "772062ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see we have this message column, so what we are going to tokenize these messages.\n",
    "# Tokenization is the word used to describe the process of what we just did , converting a normal text string into a list of \n",
    "# tokens. And tokens are just the words that we actually want or the clean version of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a100b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's go ahead and see the output of the word in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7ff24b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].head(5).apply(text_process)  # Here we are just describing the first 5 rows and then we apply the function\n",
    "                                                 # that we just created. Basically what happens is that we remove all the stop \n",
    "                                                 # words and create a list of the tokens, the important words that we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fc6b0",
   "metadata": {},
   "source": [
    "Continuing Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e5c651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of ways to continue normalizing this text. Such as Stemming , what it basically does is if our text has a bunch\n",
    "# of similar words such as 'running','ran' or 'run' these are basically just the same thing they are just different forms of \n",
    "# the word 'run', so what what stemming does is that it tries to break all of these down and just returns 'run'.The issue with\n",
    "# stemming is that we need a reference dictionary to do this and nltk comes with a lot of those built in datasets and corpuses\n",
    "# and references in order to do this easily\n",
    "# or distinguishing by part of speech.\n",
    "\n",
    "# NLTK has lots of built-in tools and great documentation on a lot of these methods. Sometimes they don't work well for\n",
    "# text-messages due to the way a lot of people tend to use abbreviations or shorthand, For example:\n",
    "\n",
    "# 'Nah dawg, IDK! Wut time u headin to da club?'\n",
    "# versus\n",
    "\n",
    "# 'No dog, I don't know! What time are you heading to the club?'\n",
    "# Some text normalization methods will have trouble with this type of shorthand and so I'll leave you to explore those more \n",
    "# advanced methods through the NLTK book online.\n",
    "\n",
    "# For now we will just focus on using what we have to convert our list of words to an actual vector that SciKit-Learn can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616c08",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "761bf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages \n",
    "# into a vector the SciKit Learn's algorithm models can work with.\n",
    "\n",
    "# Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models\n",
    "# can understand.\n",
    "\n",
    "# We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "# 1.Count how many times does a word occur in each message (Known as term frequency)\n",
    "# 2.Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "# 3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "# See actual notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc5fc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's begin Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a383c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's \n",
    "# CountVectorizer. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "# We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other \n",
    "# dimension are the actual documents, in this case a column per text message.\n",
    "\n",
    "#For example:\n",
    "\n",
    "#      Message 1 \tMessage 2\t...\tMessage N\n",
    "# Word 1 Count\t0\t1\t...\t0\n",
    "# Word 2 Count\t0\t0\t...\t0\n",
    "# ...\t1\t2\t...\t0\n",
    "# Word N Count\t0\t1\t...\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5738e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at the table in the actual notebook , we see that each column i.e Message 1, Message 2....references the actual\n",
    "# message while the rows Word1 Count , Word2 Count ..... counts how many times that word is present in that message.\n",
    "# So, for Message 1 word 1 is present 0 times in the message , word1 is present 1 time in message 2 and so on...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "79f3f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. \n",
    "# Because of this, SciKit Learn will output a Sparse Matrix.Sparse Matrices are basically matrices that have a lot of 0 values.\n",
    "# So what we want to do is that instead of storing every single value and all the elements we can store it as a sparse matrix.\n",
    "# We can basically think of this as a ton of 0's in our matrix so we are going to save it in a particular format so we actually\n",
    "# save on memory in our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35be4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "255ad692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of arguments and parameters that can be passed through the count vectorizer, and in this case we will just \n",
    "# define the analyzer to be our own previously defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "682eb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer =  CountVectorizer(analyzer=text_process).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "428a707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do bow_transformer which is bag of words transformer = CountVectorizer () and if we do shift+tab we can see that there\n",
    "# are a lot of parameters that can fit in, in order to avoid passing in all those parameters we just specify our own analyzer \n",
    "# which is the text_process function that we created earlier and then we can go ahead and fit this to my actual text data, \n",
    "# which is messages in the message column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cd45b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while as we are essentially creating a very large matrix. All the rows are words and every single word in the\n",
    "# messages and all the columns are every single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d4cb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_transformer.vocabulary_))  # This basically prints the total number of vocabulary words, which is about 11425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "968931a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we can do is that we can take one sample text message and get its bag of words count as a vector putting to use our new\n",
    "# bow_transformer. So, for example let's go ahead and make an object called mess4 and from the messages dataframe in the \n",
    "# message column we are going to grab the 4th message i.e 0,1,2,3 that's why we have 3 and then we print that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "514e2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess4 = messages['message'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e61ee6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "print(mess4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f814410",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow4 = bow_transformer.transform([mess4]) # Here we say bow4 or bag of words 4 = bow_.... and then we call the bow_transformer\n",
    "                                          # .transform and pass in that message as an item in a list, i.e mess4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c031d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n"
     ]
    }
   ],
   "source": [
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9f8b226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "print(bow4.shape)  # The shape is 1 by our entire vocabulary(11425) and this means that there are 7 unique words in message 4 or mess4\n",
    "                   # and that's after removing the common stop words.If we look at the output above we can see that (0,4068)\n",
    "                   # appears 2 times the rest only once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d0dcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can further check and see which words appear twice and the way we do that is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8738eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prach\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'U'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[4068]  # Here we use the get feature function and check for index 4068 and we get the \n",
    "                                           # letter U as a word . So, 'U' shows up twice and we can see that in this sentence\n",
    "                                           # 'U dun say so early hor... U c already then say...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00d188b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[9554] # The other word that comes up twice is at index 9554 and the word is 'say' and we \n",
    "                                          # can confirm in this sentence 'U dun say so early hor... U c already then say..'\n",
    "                                          # that it also shows up twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a bag of words transformer and we explored a example message and saw how many times a specifc word count shows up \n",
    "# in each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7e3d5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use .transform on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's \n",
    "# go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc9661b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b3421151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we say messages_bow = the entire bag of words transformer i.e bow_transformer and to that we add .transform and pass in \n",
    "# the entire message column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "166917eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have done that for all the messages we print it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de4c329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Sparse Matrix : (5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the Sparse Matrix :',messages_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3e36362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that we have 5572 rows by 11425 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e99a71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check the number of non-zero occourances , for that we just call messages_bow i.e the object we just created and \n",
    "# apply .nnz which stands for non zero occourances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebd398bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50548"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_bow.nnz  # Here we can see that we have 50,548 non zero occourances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8cfc7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also just check the sparsity if we wanted to like grabbing a formula for that, which we can get from the lecture \n",
    "# notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1cc3bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6a19df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we did above is that we basically just grabbed the non zero messages i.e messages_bow.nnz and divided it by the shape \n",
    "# or the overall number of messages i.e the rows [0] * columns [1] or messages_bow.shape[0]* messages_bow.shape[1] times 100\n",
    "# and that is the sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "75eefd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we run this we get 0 because if the round function attached to it but if we remove round we get 0.007940...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "277e461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.07940295412668218\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format((sparsity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b35c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically just comparing the number of non zero messages to the actual total number of messages which kind of gives us \n",
    "# the idea of how many zeros are actually in our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6a41cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we are done with counting the term weights and normalization can be done with TD-IDF i.e Term Frequency - Inverse\n",
    "# Document Frequency and we will use scikit-learn's TfidfTransformer to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cd314cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the details of TD-IDF from the actual lecture notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e942f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d9138efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer =  TfidfTransformer().fit(messages_bow)  # We are going to make an instance of the tfidf transformer called\n",
    "                                                           # TfidfTransformer and we are going to fit it to our bag of words\n",
    "                                                           # mesaages_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35d14a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf4 = tfidf_transformer.transform(bow4)    # We are then going to apply the transform function to tfidf_transformer to\n",
    "                                              # transform bow4 message that we created earlier just to get an idea what it looks\n",
    "                                              # like and then we print tfidf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d41b0d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c302d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see that we have an inverse document frequency and term frquency relationship for this particular message. So, we \n",
    "# have been able to transform just a simple word count into an actual TF-IDF.\n",
    "# We can bascially interpret each of these numbers as weight value for each of these words vs the actual document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "32ad6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll go ahead and check what is the IDF (inverse document frequency) of a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "09a3103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example if I wanted to check th TF_IDF for the word 'university' we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a78fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.527076498901426"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['university']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eb074f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's go ahead and convert the entire bag of words corpus into a TF-IDF corpus at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "932736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to make one more object called messages_tfidf = tfidf_transformer.transform() and within the the brackets \n",
    "# instead of just passing in a single message in the bag of words format i.e bow4 we pass in the entire thing which was\n",
    "# messages_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb260773",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c4b8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways that data can be pre processed and vectorized and these seteps involve feature enginerering and \n",
    "# building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f92b9b",
   "metadata": {},
   "source": [
    "Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ae5dba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we finally have the messages represented as numerical vectors we can finally train our spam-ham classifier.\n",
    "# We can actually pretty much use any sort of classification algorithims, but for a variety of reasons the Naive Bayes's \n",
    "# classifier is a good choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1f2befa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3cc4449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(messages_tfidf,messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "38250023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we created an object called spam_detect_model = MultinomialNB and we are going to fit this to messages_tfidf that we just\n",
    "# created and as a second argument we pass in the actual data which is messages['label.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2b273232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to try and classify our single random message and check out how we do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f492d1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype='<U4')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model.predict(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "803a5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just want the first element i.e ['ham'] from the output array so we say 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "27c9e290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model.predict(tfidf4)[0]  # As we can see my tfidf message is a ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8a904408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can go check that by grabbing our original dataframe which was messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8b87e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1      ham                      Ok lar... Joking wif u oni...      29\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3      ham  U dun say so early hor... U c already then say...      49\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      61\n",
       "...    ...                                                ...     ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...     160\n",
       "5568   ham               Will ü b going to esplanade fr home?      36\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...      57\n",
       "5570   ham  The guy did some bitching but I acted like i'd...     125\n",
       "5571   ham                         Rofl. Its true to its name      26\n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f254c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see from here the message that we are referencing is at no 3 so to check if that message is ham or spam we do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c383d1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['label'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "32e674bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So looks like we are predicting correctly.So, looks like we have created a model that can classify ham vs spam correctly.\n",
    "# If we want to run this method on all the messages in our tfidf we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f2ee336",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = spam_detect_model.predict(messages_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c0f843e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred  # Now we have the ham and spamspredictions of the  predictions of all the messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c9c87866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now something important to note here is that we trained everything on our training data. so in the above evaluation we \n",
    "# evaluated stuff on the same data that we use for training and as we know that we should never actually do that and we should\n",
    "# be splitting the data into a test set and a training set otherwise we don't know the true predictive power of our model\n",
    "# we simply remember each example during training the accuracy on the training data will be 100% even though we will not be \n",
    "# able to classify new messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6e4ae39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper way to do this as we have seen before is to use train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "362bf5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4a78401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to do this a little different from what we usually do \n",
    "# we have msg_train,..... = train_test_split() call and the features themselves i.e within the train_test_split bracket are the\n",
    "# messages['message'] and then we are going to pass in messages['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53b3e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c17225bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the train test split where we are  passing in our messages, so now lets go ahead and split up our data\n",
    "# Remember we are just grabbing the text messages themselves we haven't actually done the entire process bag of words, tfidf,\n",
    "# count vectorization etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16507962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we basically have 2 options \n",
    "# The first option is that we grab the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58d514c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1567                        I am late. I will be there at\n",
       "3497    Happy birthday... May u find ur prince charmin...\n",
       "4765    Hi.:)technical support.providing assistance to...\n",
       "4752    Your weekly Cool-Mob tones are ready to downlo...\n",
       "612     Its a valentine game. . . Send dis msg to all ...\n",
       "                              ...                        \n",
       "4655                     Hope you are having a great day.\n",
       "4461    This is wishing you a great day. Moji told me ...\n",
       "1539                             Midnight at the earliest\n",
       "1848    FREE NOKIA Or Motorola with upto 12mths 1/2pri...\n",
       "460     Going thru a very different feeling.wavering d...\n",
       "Name: message, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train  # which is just the list of all the text and then repeat all that we did earlier do the count vectorization i.e from \n",
    "           # bow_transformer =  CountVectorizer(...), do the transformation i.e bow_transform...., and do the TF-IDF i.e\n",
    "           # tfidf = tfidf_transformer().fit(...) etc... and then run multinomialNB this is one way to do it but this is such\n",
    "           # a common process with data that what scikit learn has is a datapipeline feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc367f",
   "metadata": {},
   "source": [
    "Creating a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c8bff2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second option that we can do is run our model again and predict off of our test set by using scikitlean's pipeline \n",
    "# capabilities to store an entire pipeline of our workflow.Let's see how we do this, this is what we will be doing when \n",
    "# dealing with real world text data.\n",
    "# We won't be doing the previous 2 steps that we did till now because scikit learn has a data pipeline feature which will save \n",
    "# us so much more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3ddb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "24ad6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we are going to essentially summerize all the steps we just did into a pipeline so we don't have to constantly keep\n",
    "# repeating everything for different sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2dd8cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9c2500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to make an object called pipeline which is going to be a instance of Pipeline () and what Pipeline takes in is\n",
    "# just the steps argument and we basically just pass in a list of everything that we want to do.\n",
    "# So we pass in a list we can see that there are two brackets and the first item is a tuple and the tuple takes in the name of \n",
    "# the step, in this case we call it bow and then what we actually want to do , so in this case the first step that we did was\n",
    "# CountVectorizer, with the analyzer = text_process function. So, this was the string,to token to integer counts step.\n",
    "# The next step was to take those integer counts to weighted tfidf scores- so my next tuple we'll just call it 'tfidf', we can\n",
    "# actually call this whatever we want but we'll label it correctly and call it 'tfidf' and then we call the TfidfTransformer()\n",
    "# that was integer counts for the tfidf scores and then finally we want to train this on our model and in this case we'll call\n",
    "# this 'classifier' (this is just a string name and does'nt really matter we just label it so we can reference it later and we \n",
    "# know what we are talking about) and this is what we'll call MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2508d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have created the pipeline and now we can directly pass message text data and the pipeline will do all our pre-processing\n",
    "# for us.\n",
    "# We basically just treat this pipeline model as a normal estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3faf1287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001BC339D8E50>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3696d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we grab pipeline and say .fit pass in msg_train and label_train and what its going to do is its going to do these 3 steps\n",
    "# CountVectorizer(analyzer=text_process),TfidfTransformer(),MultinomialNB() for us instead of having to it manually ourselves,\n",
    "# now we can just use pipelines to quicky and succinctly do all that. Luckily for us we just have to pass in the actual text\n",
    "# data we no longer have to worry about manually doing the Count Vectorization or the tfidf transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bbc1f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see after we run that we have created a fitted pipeline object. after which we just say :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0fe35d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)  # and pass in our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e0673a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the predictions have been completed we go ahead and do a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0e94f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "602044da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1455\n",
      "        spam       1.00      0.68      0.81       217\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.98      0.84      0.89      1672\n",
      "weighted avg       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "69366cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that we get a 96% accuracy for precision and recall which is pretty good considering that we are just dealing with\n",
    "# straight text data. SO python has the ability to deal with text data using scikit learn. \n",
    "# So we have a classification report for a model on a true testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "572ece15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP is a vast topic and has much more than what we just covered, feel free to read more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e743e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now in the pipeline that we created, supose we don't want to use the MultinomialNB classifier we can use other classifers \n",
    "# Let's say we want to do RandomForest in this we can do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "de183680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6964ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and instead of MultinomialNB classifier we replace that with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f70a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we run all the steps following that and we can see how it compares against the Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ccf41aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001BC339D8E50>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4255daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a2d8670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1455\n",
      "        spam       0.99      0.76      0.86       217\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.98      0.88      0.92      1672\n",
      "weighted avg       0.97      0.97      0.97      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "949dc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the accuracy is around the same maybe a little better but it changed around the F1 ad recall score a bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the pipeline creation was the most important thing that we did here. When we are working with real world text \n",
    "# data we won't do all these steps that we did prior, in the real world we build these pipeline for ourselves, do the\n",
    "# CountVectorization, do the transformation and call whatever model we want. For text data its usually MutinomialNB, \n",
    "# then just call the pipeline, do pipeline.fit...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
